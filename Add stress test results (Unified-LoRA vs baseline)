## ğŸ”¬ Stress Test on Tinker's LoRA API (Unified-LoRA vs Fixed-LR Baseline)

To evaluate whether the Unified-LoRA controller provides practical benefits during
online LoRA training, I performed a controlled stress test using Tinkerâ€™s  
`meta-llama/Llama-3.2-1B` LoRA API.

The setup:

- Task: toy Pig-Latin translation  
- Two datasets: **clean** (normal) and **corrupted** (shock)  
- Two synthetic shock windows: **[200â€“300]** and **[500â€“600]**  
- Unified-LoRA controller:  
  - Modes: **Single â†’ Multi â†’ Mirror**  
  - LR: **2e-3 â†’ 5e-4 â†’ 1e-4**  
  - Stress signal Ï• computed from smoothed error *Eâ‚›*  
- Baseline: standard LoRA with **fixed LR = 5e-4**

---

## ğŸ“ˆ 1. Loss Dynamics Under Shock

### Unified-LoRA (adaptive)
| Step | Shock | Loss     | Mode | LR       |
|------|--------|----------|------|----------|
| 200  | Yes    | 18.42    | Single â†’ Multi | â†“ |
| 225  | Yes    | 2.56     | Multi          | 5e-4 |
| 250  | Yes    | 0.0015   | Multi          | 5e-4 |
| 275  | Yes    | 0.0010   | Mirror         | 1e-4 |
| 300  | No     | 4.27     | Mirror         | 1e-4 |
| 350  | No     | **0.0004** | Multi â†’ Single | â†‘ |

â¡ï¸ **Shock absorbed quickly; full recovery by step ~350.**  
â¡ï¸ No large overshoots after shock ends.

---

### Baseline (fixed LR = 5e-4)
| Step | Shock | Loss     |
|------|--------|----------|
| 200  | Yes    | 9.28     |
| 225  | Yes    | 1.89     |
| 250  | Yes    | 3.43 â¬…ï¸ rebound |
| 275  | Yes    | 0.10     |
| 300  | No     | **13.09** â¬…ï¸ massive overshoot |
| 350  | No     | 3.70     |
| 600  | No     | 11.45 (after second shock) |

â¡ï¸ **Recovery is unstable and significantly slower.**  
â¡ï¸ Large overshoots even *after* the shock window ends.

---

## ğŸ§  2. What the Test Demonstrates

### âœ… Unified-LoRA adapts to stress  
The controller switches modes based on the stress signal Ï•:
``Single â†’ Multi â†’ Mirror``  
with progressively smaller learning rates.

### âœ… Unified-LoRA stabilizes training faster  
In both shock windows, Unified-LoRA suppresses the loss to ~0.001 within ~50 steps
and returns to stable training shortly after the shock ends.

### âŒ Baseline (fixed LR) is fragile  
It shows:
- repeated overshoots  
- unstable behavior after shock windows  
- slow return to low loss values  

### ğŸ¯ Conclusion  
**Unified-LoRA improves robustness during online LoRA training.**  
It reacts to distribution shifts automatically and maintains stability,  
while a fixed-LR LoRA setup exhibits large instabilities and delayed recovery.

---

## ğŸ“ Code Availability

The exact scripts used for the stress test are available in `stress_test/`  
and integrate directly with Tinkerâ€™s LoRA API (`create_lora_training_client`).

